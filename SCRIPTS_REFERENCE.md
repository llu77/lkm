# ğŸ“š Ù…Ø±Ø¬Ø¹ Scripts - Ø¯Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…

## ğŸ¯ Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

Scripts Python Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ØªÙˆÙØ±:
- âœ… ØªÙƒØ§Ù…Ù„ Ù…Ø¹ AWS Bedrock Claude
- âœ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ (Caching + Batching)
- âœ… ØªÙƒØ§Ù…Ù„ Ù…Ø¹ S3
- âœ… Ø¯ÙˆØ§Ù„ Lambda Ø¬Ø§Ù‡Ø²Ø©

---

## ğŸ“ Ø§Ù„Ø¨Ù†ÙŠØ©

```
scripts/
â”œâ”€â”€ claude_bedrock/          # ØªÙƒØ§Ù…Ù„ AWS Bedrock
â”‚   â”œâ”€â”€ inference_adapter.py       # Ù…Ø¨Ø§Ø´Ø±
â”‚   â”œâ”€â”€ optimized_adapter.py       # Ù…Ø­Ø³Ù‘Ù†
â”‚   â””â”€â”€ s3_adapter.py              # S3
â”œâ”€â”€ performance/             # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
â”‚   â””â”€â”€ optimizer.py               # Cache + Batch
â”œâ”€â”€ lambda/                  # AWS Lambda functions
â”‚   â””â”€â”€ contextual_retrieval_handler.py
â””â”€â”€ config/
    â””â”€â”€ performance.json           # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
```

---

## 1ï¸âƒ£ AWS Bedrock Claude Integration

### **InferenceAdapter** (Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ)

#### Ø§Ù„ÙˆØ¸Ø§Ø¦Ù:
```python
from scripts.claude_bedrock import InferenceAdapter

adapter = InferenceAdapter(
    region_name='us-east-1',
    model_id='anthropic.claude-haiku-4-5-20251001-v1:0'
)
```

#### **Ø£) Streaming Response:**
```python
# Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ Ù…Ø¨Ø§Ø´Ø±Ø© chunk by chunk
for chunk in adapter.invoke_model_with_response_stream(
    prompt="Ø§Ø´Ø±Ø­ Ù„ÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
    max_tokens=1000,
    temperature=0.0
):
    print(chunk, end='', flush=True)
```

#### **Ø¨) Complete Response:**
```python
# Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©
response = adapter.invoke_model(
    prompt="Ù…Ø§ Ù‡Ùˆ 2+2ØŸ",
    max_tokens=1000,
    temperature=0.0
)
print(response)  # "4"
```

#### **Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:**
- `prompt`: Ø§Ù„Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø§Ù„Ø·Ù„Ø¨ (str)
- `max_tokens`: Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ tokens (int, default: 1000)
- `temperature`: 0.0 (Ø¯Ù‚ÙŠÙ‚) Ø¥Ù„Ù‰ 1.0 (Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ) (float)

#### **Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©:**
- `anthropic.claude-haiku-4-5-20251001-v1:0` - Ø³Ø±ÙŠØ¹ ÙˆØ±Ø®ÙŠØµ
- `anthropic.claude-sonnet-4-5-20250929-v1:0` - Ù…ØªÙˆØ§Ø²Ù† (Ù…ÙˆØµÙ‰ Ø¨Ù‡)
- `anthropic.claude-opus-4-20250514-v1:0` - Ø§Ù„Ø£Ù‚ÙˆÙ‰

---

## 2ï¸âƒ£ OptimizedInferenceAdapter (Ø§Ù„Ù…Ø­Ø³Ù‘Ù†)

### **Ø§Ù„Ù…Ø²Ø§ÙŠØ§:**
âœ… **Caching** - ØªÙˆÙÙŠØ± 90% Ù…Ù† Ø§Ù„ØªÙƒÙ„ÙØ©
âœ… **TTL** - Ø§Ù†ØªÙ‡Ø§Ø¡ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù€ cache
âœ… **LRU Eviction** - Ø¥Ø¯Ø§Ø±Ø© Ø°ÙƒÙŠØ© Ù„Ù„Ø°Ø§ÙƒØ±Ø©
âœ… **Batching** - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø§Øª

### **Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:**

```python
from scripts.claude_bedrock import OptimizedInferenceAdapter

adapter = OptimizedInferenceAdapter(
    enable_cache=True,      # ØªÙØ¹ÙŠÙ„ Cache
    enable_batching=False,  # Batching (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    config_path='config/performance.json'  # Ø§Ø®ØªÙŠØ§Ø±ÙŠ
)
```

### **Ø£) Ù…Ø¹ Caching:**

```python
# Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ - ÙŠØªØµÙ„ Ø¨Ù€ API (Ø¨Ø·ÙŠØ¡)
response1 = adapter.invoke_model_cached("Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ")
print(f"Ø£ÙˆÙ„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡: {len(response1)} Ø­Ø±Ù")

# Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© - Ù…Ù† Ø§Ù„Ù€ Cache (Ø³Ø±ÙŠØ¹ Ø¬Ø¯Ø§Ù‹ + Ù…Ø¬Ø§Ù†ÙŠ!)
response2 = adapter.invoke_model_cached("Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ")
print(f"Ø«Ø§Ù†ÙŠ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ (cached): {len(response2)} Ø­Ø±Ù")

# Ù†ÙØ³ Ø§Ù„Ù†ØªÙŠØ¬Ø©ØŒ Ù„ÙƒÙ† Ø¨Ø¯ÙˆÙ† ØªÙƒÙ„ÙØ© API!
```

### **Ø¨) Force Refresh:**

```python
# Ø¥Ø¬Ø¨Ø§Ø± ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù€ Cache
response = adapter.invoke_model_cached(
    prompt="Ø§Ù„Ø³Ø¤Ø§Ù„",
    force_refresh=True  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù€ Cache ÙˆØ§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù†ØªÙŠØ¬Ø© Ø¬Ø¯ÙŠØ¯Ø©
)
```

### **Ø¬) Batch Processing:**

```python
# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø¯Ø© Ø£Ø³Ø¦Ù„Ø© Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©
requests = [
    {"prompt": "Ù…Ø§ Ù‡Ùˆ AIØŸ", "max_tokens": 100},
    {"prompt": "Ù…Ø§ Ù‡Ùˆ MLØŸ", "max_tokens": 100},
    {"prompt": "Ù…Ø§ Ù‡Ùˆ DLØŸ", "max_tokens": 100},
]

results = adapter.invoke_batch(requests, use_cache=True)

for i, result in enumerate(results):
    print(f"Ø§Ù„Ø³Ø¤Ø§Ù„ {i+1}: {result[:50]}...")
```

### **Ø¯) Cache Management:**

```python
# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù€ Cache
stats = adapter.get_cache_stats()
print(f"Cache size: {stats['size']}/{stats['max_entries']}")
print(f"TTL: {stats['ttl_ms']}ms")

# ØªÙ†Ø¸ÙŠÙ Cache Ø§Ù„Ù…Ù†ØªÙ‡ÙŠ
removed = adapter.cleanup_expired_cache()
print(f"Removed {removed} expired entries")

# Ù…Ø³Ø­ ÙƒÙ„ Ø§Ù„Ù€ Cache
adapter.clear_cache()
```

---

## 3ï¸âƒ£ Performance Optimizer

### **PredictionCache** (Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª)

```python
from scripts.performance import PredictionCache

cache = PredictionCache(
    max_entries=10000,  # Ø£Ù‚ØµÙ‰ 10,000 Ø¥Ø¯Ø®Ø§Ù„
    ttl_ms=300000       # 5 Ø¯Ù‚Ø§Ø¦Ù‚ TTL
)

# Ø­ÙØ¸
cache.set("my_key", "my_value")

# Ù‚Ø±Ø§Ø¡Ø©
value = cache.get("my_key")

# Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
print(f"Cache size: {cache.size()}")

# ØªÙ†Ø¸ÙŠÙ
expired = cache.cleanup_expired()
```

### **RequestBatcher** (Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø§Øª)

```python
from scripts.performance import RequestBatcher

def process_batch(batch):
    print(f"Processing {len(batch)} requests")
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø©

batcher = RequestBatcher(
    max_batch_size=100,      # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 100 Ø·Ù„Ø¨
    max_wait_time_ms=50,     # Ø§Ù†ØªØ¸Ø§Ø± 50ms ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰
    callback=process_batch   # Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
)

# Ø¥Ø¶Ø§ÙØ© Ø·Ù„Ø¨Ø§Øª
batcher.add({"prompt": "Question 1"})
batcher.add({"prompt": "Question 2"})

# ÙŠÙØ³ØªØ¯Ø¹Ù‰ callback ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¹Ù†Ø¯ Ø§Ù…ØªÙ„Ø§Ø¡ Ø§Ù„Ø¯ÙØ¹Ø© Ø£Ùˆ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ÙˆÙ‚Øª
```

### **PerformanceOptimizer** (Ø§Ù„Ø´Ø§Ù…Ù„)

```python
from scripts.performance import PerformanceOptimizer

optimizer = PerformanceOptimizer(
    config_path='config/performance.json'  # Ø§Ø®ØªÙŠØ§Ø±ÙŠ
)

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Cache Ù…Ø¹ compute fallback
result = optimizer.get_cached_or_compute(
    key="my_expensive_operation",
    compute_fn=lambda: expensive_operation(),
    force_refresh=False
)
```

---

## 4ï¸âƒ£ S3 Integration

```python
from scripts.claude_bedrock import S3Adapter

s3 = S3Adapter(region_name='us-east-1')

# Ù‚Ø±Ø§Ø¡Ø© JSON
data = s3.read_from_s3('my-bucket', 'data.json')

# ÙƒØªØ§Ø¨Ø© JSON
s3.write_output_to_s3('my-bucket', 'output.json', {'key': 'value'})

# Ù‚Ø±Ø§Ø¡Ø© Bytes
image_bytes = s3.read_bytes_from_s3('my-bucket', 'image.png')

# ÙƒØªØ§Ø¨Ø© Bytes
s3.write_bytes_to_s3(
    'my-bucket',
    'output.png',
    image_bytes,
    content_type='image/png'
)
```

---

## 5ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†

### **config/performance.json:**

```json
{
  "predictionCache": {
    "enabled": true,
    "maxEntries": 10000,    // Ø­Ø¯ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„Ø§Øª
    "ttl": 300000           // 5 Ø¯Ù‚Ø§Ø¦Ù‚ Ø¨Ø§Ù„Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ©
  },
  "batching": {
    "enabled": true,
    "maxBatchSize": 100,    // Ø­Ø¯ Ø§Ù„Ø¯ÙØ¹Ø©
    "maxWaitTime": 50       // Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø§Ù„Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ©
  },
  "wasm": {
    "simd": true,
    "threads": 4,
    "memoryPages": 256
  }
}
```

---

## ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡

| Ø§Ù„Ù…ÙŠØ²Ø© | Ø¹Ø§Ø¯ÙŠ | Ù…Ø¹ Caching | Ø§Ù„ØªØ­Ø³ÙŠÙ† |
|--------|------|-----------|---------|
| **Ø§Ù„ÙˆÙ‚Øª** | 2000ms | 5ms | **99.75% Ø£Ø³Ø±Ø¹** |
| **Ø§Ù„ØªÙƒÙ„ÙØ©** | $0.10 | $0.00 | **Ù…Ø¬Ø§Ù†ÙŠ!** |
| **API Calls** | ÙƒÙ„ Ù…Ø±Ø© | Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© | **90% Ø£Ù‚Ù„** |

---

## ğŸ¯ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ÙŠØ©

### **Case 1: AI Assistant ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚**

```python
from scripts.claude_bedrock import OptimizedInferenceAdapter

# ÙÙŠ src/lib/ai-helper.ts (Ø£Ùˆ .py Ù„Ù„Ù€ backend)
adapter = OptimizedInferenceAdapter(enable_cache=True)

def ask_ai(question: str) -> str:
    """
    Ø³Ø¤Ø§Ù„ Ø§Ù„Ù€ AI Ù…Ø¹ caching ØªÙ„Ù‚Ø§Ø¦ÙŠ
    """
    return adapter.invoke_model_cached(
        prompt=f"Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: {question}",
        max_tokens=500,
        temperature=0.3
    )

# Ø§Ø³ØªØ®Ø¯Ø§Ù…
answer = ask_ai("ÙƒÙŠÙ Ø£Ø­Ø³Ø¨ Ø§Ù„Ø±ÙˆØ§ØªØ¨ØŸ")
```

### **Case 2: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø§Øª Ø§Ù„Ù…ÙˆØ¸ÙÙŠÙ†**

```python
# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©
employee_requests = [
    {"prompt": f"ØªØ­Ù„ÙŠÙ„ Ø·Ù„Ø¨ Ø§Ù„Ù…ÙˆØ¸Ù {emp}", "max_tokens": 200}
    for emp in employees
]

results = adapter.invoke_batch(employee_requests, use_cache=True)

for emp, result in zip(employees, results):
    print(f"{emp}: {result}")
```

### **Case 3: ØªÙ‚Ø§Ø±ÙŠØ± Ø°ÙƒÙŠØ©**

```python
def generate_smart_report(data):
    """
    ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AI Ù…Ø¹ caching
    """
    prompt = f"""
    Ø£Ù†Ø´Ø¦ ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ÙŠ Ø¹Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:
    Ø§Ù„Ø¥ÙŠØ±Ø§Ø¯Ø§Øª: {data['revenues']}
    Ø§Ù„Ù…ØµØ±ÙˆÙØ§Øª: {data['expenses']}

    Ù‚Ø¯Ù‘Ù… ØªØ­Ù„ÙŠÙ„ ÙˆØªÙˆØµÙŠØ§Øª.
    """

    return adapter.invoke_model_cached(
        prompt=prompt,
        max_tokens=1000,
        temperature=0.5
    )
```

---

## âš™ï¸ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª

### **1. AWS Credentials:**

```bash
# ÙÙŠ ~/.aws/credentials
[default]
aws_access_key_id = YOUR_KEY
aws_secret_access_key = YOUR_SECRET
region = us-east-1
```

Ø£Ùˆ:

```bash
# Environment Variables
export AWS_ACCESS_KEY_ID=xxx
export AWS_SECRET_ACCESS_KEY=xxx
export AWS_DEFAULT_REGION=us-east-1
```

### **2. IAM Permissions:**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel",
        "bedrock:InvokeModelWithResponseStream"
      ],
      "Resource": "arn:aws:bedrock:*::foundation-model/anthropic.claude-*"
    }
  ]
}
```

### **3. Ø§Ù„ØªØ«Ø¨ÙŠØª:**

```bash
cd scripts
pip install -r requirements.txt
```

---

## ğŸš€ Ø§Ù„Ø®Ù„Ø§ØµØ©

### **Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„ ÙˆØ§Ø­Ø¯:**

| Ø§Ù„Ø­Ø§Ù„Ø© | Ø§Ù„Ø£Ø¯Ø§Ø© |
|-------|--------|
| Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¨Ø³ÙŠØ· ÙˆØ§Ø­Ø¯ | `InferenceAdapter` |
| Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ù…ØªÙƒØ±Ø±Ø© | `OptimizedInferenceAdapter` |
| Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø§Øª | `invoke_batch()` |
| S3 operations | `S3Adapter` |
| Cache Ù…Ø®ØµØµ | `PredictionCache` |

### **Best Practices:**

1. âœ… **Ø§Ø³ØªØ®Ø¯Ù… Caching Ø¯Ø§Ø¦Ù…Ø§Ù‹** Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
2. âœ… **Batching** Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
3. âœ… **Ù…Ø±Ø§Ù‚Ø¨Ø© Cache stats** Ù„Ø¶Ø¨Ø· Ø§Ù„Ø£Ø¯Ø§Ø¡
4. âœ… **TTL Ù…Ù†Ø§Ø³Ø¨** (5 Ø¯Ù‚Ø§Ø¦Ù‚ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØºÙŠØ±Ø©ØŒ Ø£Ø·ÙˆÙ„ Ù„Ù„Ø«Ø§Ø¨ØªØ©)
5. âœ… **Cleanup Ø¯ÙˆØ±ÙŠ** Ù„Ù„Ù€ expired entries

---

## ğŸ“ Ø£Ù…Ø«Ù„Ø© Ø³Ø±ÙŠØ¹Ø© Ù„Ù„Ù†Ø³Ø® ÙˆØ§Ù„Ù„ØµÙ‚

### Ù…Ø«Ø§Ù„ 1: Ø³Ø¤Ø§Ù„ Ø¨Ø³ÙŠØ·
```python
from scripts.claude_bedrock import InferenceAdapter
adapter = InferenceAdapter()
print(adapter.invoke_model("Ù…Ø§ Ù‡Ùˆ 2+2ØŸ"))
```

### Ù…Ø«Ø§Ù„ 2: Ù…Ø¹ Caching
```python
from scripts.claude_bedrock import OptimizedInferenceAdapter
adapter = OptimizedInferenceAdapter(enable_cache=True)
print(adapter.invoke_model_cached("Ø´Ø±Ø­ AI"))
```

### Ù…Ø«Ø§Ù„ 3: Batch
```python
requests = [{"prompt": f"Q{i}", "max_tokens": 100} for i in range(5)]
results = adapter.invoke_batch(requests)
```

---

**âœ… Scripts Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©!**
