"""
AWS Bedrock Inference Adapter for Claude Models
================================================

This module provides a simple interface for invoking Claude models via AWS Bedrock
with streaming response support.

Usage:
    from claude_bedrock import InferenceAdapter

    adapter = InferenceAdapter()
    for chunk in adapter.invoke_model_with_response_stream("Hello, Claude!"):
        print(chunk, end='', flush=True)
"""

import json
import boto3
from typing import Generator, Optional
from botocore.exceptions import ClientError


class InferenceAdapter:
    """
    Adapter for invoking Claude models via AWS Bedrock with streaming responses.

    Attributes:
        bedrock_runtime: Boto3 Bedrock runtime client
        model_id: Claude model identifier for Bedrock
    """

    def __init__(self, region_name: str = 'us-east-1', model_id: Optional[str] = None):
        """
        Initialize the InferenceAdapter.

        Args:
            region_name: AWS region name (default: 'us-east-1')
            model_id: Claude model ID (default: Claude Haiku 4.5)
        """
        self.bedrock_runtime = boto3.client(
            service_name='bedrock-runtime',
            region_name=region_name
        )
        self.model_id = model_id or 'anthropic.claude-haiku-4-5-20251001-v1:0'

    def invoke_model_with_response_stream(
        self,
        prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.0
    ) -> Generator[str, None, None]:
        """
        Invoke Claude model with streaming response.

        Args:
            prompt: The user prompt to send to Claude
            max_tokens: Maximum tokens to generate (default: 1000)
            temperature: Sampling temperature 0.0-1.0 (default: 0.0)

        Yields:
            str: Text chunks as they are generated by Claude

        Example:
            >>> adapter = InferenceAdapter()
            >>> for chunk in adapter.invoke_model_with_response_stream("Hello!"):
            ...     print(chunk, end='', flush=True)
        """
        request_body = json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": temperature,
        })

        # Invoke the model
        try:
            response = self.bedrock_runtime.invoke_model_with_response_stream(
                modelId=self.model_id,
                contentType='application/json',
                accept='application/json',
                body=request_body
            )

            for event in response.get('body'):
                chunk = json.loads(event['chunk']['bytes'].decode())
                if chunk['type'] == 'content_block_delta':
                    yield chunk['delta']['text']
                elif chunk['type'] == 'message_delta':
                    if 'stop_reason' in chunk['delta']:
                        break

        except ClientError as e:
            print(f"An error occurred: {e}")
            yield None

    def invoke_model(
        self,
        prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.0
    ) -> Optional[str]:
        """
        Invoke Claude model and return the complete response.

        Args:
            prompt: The user prompt to send to Claude
            max_tokens: Maximum tokens to generate (default: 1000)
            temperature: Sampling temperature 0.0-1.0 (default: 0.0)

        Returns:
            str: Complete response from Claude, or None if error occurs

        Example:
            >>> adapter = InferenceAdapter()
            >>> response = adapter.invoke_model("What is 2+2?")
            >>> print(response)
        """
        chunks = []
        for chunk in self.invoke_model_with_response_stream(prompt, max_tokens, temperature):
            if chunk is not None:
                chunks.append(chunk)
            else:
                return None
        return ''.join(chunks)
